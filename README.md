# ppo-results
[![CC BY-SA 4.0][cc-by-sa-shield]][cc-by-sa]

This repository contains all the episode reward graphs and final score tables generated for my master's thesis on
Proximal Policy Optimization. The code used to run my experiments can be found at
https://github.com/Aehiles/ppo-pytorch. I conducted experiments on BeamRider, Breakout, Pong, Seaquest and
SpaceInvaders.

Each directory contains a dedicated experiment testing a specific optimization or hyperparameter choice. Underscores in
directory names mostly denote decimal dots when they are inbetween two numbers, e.g., `epsilon_0_2` contains the results
for an experiment that was run with the hyperparameter epsilon set to 0.2. The exception to this are the `c1_` and `c2_`
directories, since c1 and c2 denote two hyperparameters.

For each experiment, at least two graphs are available. Image names ending on `_reference` contain a magenta trendline
generated from the reference configuration. Image names ending on `_no_optim` contain a magenta trendline generated from
the no optimizations experiment.

# Graphs
The graphs are episode reward graphs. Each point marks the end of an episode at the marked time step (x axis) with the
marked score (y axis). Each agent was trained on 10 million time steps.

Scatter plots were smoothed by computing a mean on a sliding window of size 16 that is centered so each data point
is the average of the 8 previous and the 7 following data points. Trendlines were generated by combining the data from
all three runs. The smoothing used for the trendline follows the same method with a sliding window of size 256.

# LICENSE

This work is licensed under a [Creative Commons Attribution-ShareAlike 4.0
International License][cc-by-sa].

[![CC BY-SA 4.0][cc-by-sa-image]][cc-by-sa]

[cc-by-sa]: http://creativecommons.org/licenses/by-sa/4.0/
[cc-by-sa-image]: https://licensebuttons.net/l/by-sa/4.0/88x31.png
[cc-by-sa-shield]: https://img.shields.io/badge/License-CC%20BY--SA%204.0-lightgrey.svg
